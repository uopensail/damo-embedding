"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[333],{3905:(n,e,t)=>{t.d(e,{Zo:()=>p,kt:()=>u});var r=t(7294);function i(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function a(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,r)}return t}function s(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?a(Object(t),!0).forEach((function(e){i(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function o(n,e){if(null==n)return{};var t,r,i=function(n,e){if(null==n)return{};var t,r,i={},a=Object.keys(n);for(r=0;r<a.length;r++)t=a[r],e.indexOf(t)>=0||(i[t]=n[t]);return i}(n,e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);for(r=0;r<a.length;r++)t=a[r],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(i[t]=n[t])}return i}var d=r.createContext({}),l=function(n){var e=r.useContext(d),t=e;return n&&(t="function"==typeof n?n(e):s(s({},e),n)),t},p=function(n){var e=l(n.components);return r.createElement(d.Provider,{value:e},n.children)},m="mdxType",c={inlineCode:"code",wrapper:function(n){var e=n.children;return r.createElement(r.Fragment,{},e)}},f=r.forwardRef((function(n,e){var t=n.components,i=n.mdxType,a=n.originalType,d=n.parentName,p=o(n,["components","mdxType","originalType","parentName"]),m=l(t),f=i,u=m["".concat(d,".").concat(f)]||m[f]||c[f]||a;return t?r.createElement(u,s(s({ref:e},p),{},{components:t})):r.createElement(u,s({ref:e},p))}));function u(n,e){var t=arguments,i=e&&e.mdxType;if("string"==typeof n||i){var a=t.length,s=new Array(a);s[0]=f;var o={};for(var d in e)hasOwnProperty.call(e,d)&&(o[d]=e[d]);o.originalType=n,o[m]="string"==typeof n?n:i,s[1]=o;for(var l=2;l<a;l++)s[l]=t[l];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}f.displayName="MDXCreateElement"},8125:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>s,default:()=>c,frontMatter:()=>a,metadata:()=>o,toc:()=>l});var r=t(7462),i=(t(7294),t(3905));const a={},s="Example",o={unversionedId:"Example",id:"Example",title:"Example",description:"Embedding",source:"@site/docs/Example.md",sourceDirName:".",slug:"/Example",permalink:"/damo-embedding/docs/Example",draft:!1,editUrl:"https://github.com/uopensail/damo-embedding/edit/docs/docs/docs/Example.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",next:{title:"Damo-Embedding",permalink:"/damo-embedding/docs/Intro"}},d={},l=[{value:"Embedding",id:"embedding",level:2},{value:"DeepFM",id:"deepfm",level:2}],p={toc:l},m="wrapper";function c(n){let{components:e,...t}=n;return(0,i.kt)(m,(0,r.Z)({},p,t,{components:e,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"example"},"Example"),(0,i.kt)("h2",{id:"embedding"},"Embedding"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\n\nimport damo\nimport torch\nimport numpy as np\nfrom typing import Union\nfrom collections import defaultdict\n\n\nclass Storage(object):\n    """singleton storage class."""\n\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = object.__new__(cls)\n            cls._instance.dir = kwargs.get("dir", "./embeddings")\n            cls._instance.ttl = kwargs.get("ttl", 8640000)\n            cls._instance.storage = damo.PyStorage(cls._instance.dir, cls._instance.ttl)\n        return cls._instance\n\n    @staticmethod\n    def checkpoint(path: str):\n        assert Storage._instance is not None\n        Storage._instance.storage.checkpoint(path)\n\n    @staticmethod\n    def dump(path: str):\n        assert Storage._instance is not None\n        Storage._instance.storage.dump(path)\n\n    @staticmethod\n    def load_from_checkpoint(path: str):\n        assert Storage._instance is not None\n        Storage._instance.storage.load_from_checkpoint(path)\n\n\nclass Embedding(torch.nn.Module):\n    _group = -1\n\n    def __init__(self, dim: int, initializer={}, optimizer={}, group=-1, **kwargs):\n        super(Embedding, self).__init__()\n        self.dim = dim\n        if group != -1:\n            self.group = group\n            assert 0 <= self.group < 256\n        else:\n            Embedding._group += 1\n            self.group = Embedding._group\n            assert 0 <= self.group < 256\n        self.storage = Storage(**kwargs).storage\n\n        # create initializer\n        init_params = damo.Parameters()\n        for k, v in initializer.items():\n            init_params.insert(k, v)\n        self.initializer = damo.PyInitializer(init_params)\n\n        # create optimizer\n        opt_params = damo.Parameters()\n        for k, v in optimizer.items():\n            opt_params.insert(k, v)\n        self.optimizer = damo.PyOptimizer(opt_params)\n\n        self.embedding = damo.PyEmbedding(\n            self.storage, self.optimizer, self.initializer, self.dim, self.group\n        )\n\n    def forward(self, inputs: Union[torch.Tensor, np.ndarray]) -> torch.Tensor:\n        """embedding lookup\n\n        Args:\n            inputs (Union[torch.Tensor, np.ndarray]): input values\n\n        Returns:\n            torch.Tensor: embedding values (inputs.shape[0], inputs.shape[1], self.dim)\n        """\n\n        data = inputs\n        if isinstance(inputs, torch.Tensor):\n            data = inputs.numpy().astype(np.uint64)\n        elif isinstance(inputs, np.ndarray):\n            if data.type != np.uint64:\n                data = inputs.astype(np.uint64)\n\n        batch_size, width = data.shape\n        keys = np.unique(np.concatenate(data)).astype(np.uint64)\n        length = keys.shape[0]\n        weights = np.zeros(length * self.dim, dtype=np.float32)\n        self.embedding.lookup(keys, weights)\n        weights = weights.reshape((length, self.dim))\n        weight_dict = {k: v for k, v in zip(keys, weights)}\n        values = np.zeros(shape=(batch_size, width, self.dim), dtype=np.float32)\n\n        for i in range(batch_size):\n            for j in range(width):\n                key = data[i][j]\n                # 0 is padding value\n                if key != 0:\n                    values[i][j] = weight_dict[key]\n\n        def apply_gradients(gradients):\n            grad = gradients.numpy()\n            grad = grad.reshape((batch_size, width, self.dim))\n            grad_dict = defaultdict(lambda: np.zeros(self.dim, dtype=np.float32))\n            for i in range(batch_size):\n                for j in range(width):\n                    key = data[i][j]\n                    if key != 0:\n                        grad_dict[key] += grad[i][j]\n\n            values = np.zeros(length * self.dim, dtype=np.float32)\n            for i in range(length):\n                values[i * self.dim : (i + 1) * self.dim] = (\n                    grad_dict[keys[i]] / batch_size\n                )\n\n            self.embedding.apply_gradients(keys, values)\n\n        ret = torch.from_numpy(values)\n        ret.requires_grad_()\n        ret.register_hook(apply_gradients)\n        return ret\n\n\n')),(0,i.kt)("h2",{id:"deepfm"},"DeepFM"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Union\nfrom embedding import Embedding\n\n\nclass DeepFM(torch.nn.Module):\n    def __init__(\n        self,\n        emb_size: int,\n        fea_size: int,\n        hid_dims=[256, 128],\n        num_classes=1,\n        dropout=[0.2, 0.2],\n        **kwargs,\n    ):\n        super(DeepFM, self).__init__()\n        self.emb_size = emb_size\n        self.fea_size = fea_size\n\n        initializer = {\n            "name": "truncate_normal",\n            "mean": float(kwargs.get("mean", 0.0)),\n            "stddev": float(kwargs.get("stddev", 0.0001)),\n        }\n\n        optimizer = {\n            "name": "adam",\n            "gamma": float(kwargs.get("gamma", 0.001)),\n            "beta1": float(kwargs.get("beta1", 0.9)),\n            "beta2": float(kwargs.get("beta2", 0.999)),\n            "lambda": float(kwargs.get("lambda", 0.0)),\n            "epsilon": float(kwargs.get("epsilon", 1e-8)),\n        }\n\n        self.w = Embedding(\n            1,\n            initializer=initializer,\n            optimizer=optimizer,\n            group=0,\n            **kwargs,\n        )\n\n        self.v = Embedding(\n            self.emb_size,\n            initializer=initializer,\n            optimizer=optimizer,\n            group=1,\n            **kwargs,\n        )\n        self.w0 = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n        self.dims = [fea_size * emb_size] + hid_dims\n\n        self.layers = nn.ModuleList()\n        for i in range(1, len(self.dims)):\n            self.layers.append(nn.Linear(self.dims[i - 1], self.dims[i]))\n            self.layers.append(nn.BatchNorm1d(self.dims[i]))\n            self.layers.append(nn.BatchNorm1d(self.dims[i]))\n            self.layers.append(nn.ReLU())\n            self.layers.append(nn.Dropout(dropout[i - 1]))\n        self.layers.append(nn.Linear(self.dims[-1], num_classes))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inputs: Union[torch.Tensor, np.ndarray]) -> torch.Tensor:\n        """forward\n\n        Args:\n            inputs (Union[torch.Tensor, np.ndarray]): input tensor\n\n        Returns:\n            tensor.Tensor: deepfm forward values\n        """\n        assert inputs.shape[1] == self.fea_size\n        w = self.w.forward(inputs)\n        v = self.v.forward(inputs)\n        square_of_sum = torch.pow(torch.sum(v, dim=1), 2)\n        sum_of_square = torch.sum(v * v, dim=1)\n        fm_out = (\n            torch.sum((square_of_sum - sum_of_square) * 0.5, dim=1, keepdim=True)\n            + torch.sum(w, dim=1)\n            + self.w0\n        )\n\n        dnn_out = torch.flatten(v, 1)\n        for layer in self.layers:\n            dnn_out = layer(dnn_out)\n        out = fm_out + dnn_out\n        out = self.sigmoid(out)\n        return out\n\n')))}c.isMDXComponent=!0}}]);