#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# `Damo-Embedding` - 'c++ tool for sparse parameter server'
# Copyright (C) 2019 - present timepi <timepi123@gmail.com>
# `Damo-Embedding` is provided under: GNU Affero General Public License
# (AGPL3.0) https:#www.gnu.org/licenses/agpl-3.0.html unless stated otherwise.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#

import copy
import json
import shutil
import struct
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import torch
import torch.onnx

from .damo_helper import Embedding, dump


def sparse_to_numpy(
    sparse_path: str,
) -> Tuple[Dict[int, np.ndarray], Dict[int, Dict[int, int]]]:
    """!
    @brief Parses sparse binary data into numpy format

    @details Processes binary sparse data files generated by the Storage module,
    converting them into usable embedding data structures.

    @param sparse_path Path to input sparse data file
    @return Tuple containing:
        - group_data: Dictionary of embedding data arrays {group_id: numpy_array}
        - group_ids: Nested dictionary of index mappings {group_id: {original_id: mapped_index}}

    @note File format specification:
        - First 4 bytes: number of groups (int32)
        - groups array: group IDs (int32 * size)
        - dims array: dimensions per group (int32 * size)
        - counts array: record counts per group (int64 * size)
        - Data records:
            - 8-byte key (int64)
            - 4-byte group ID (int32)
            - 4*dim bytes of weights (float32 * dim)
    """
    group_data = {}
    group_ids = {}
    group_info = {}  # {group: (dim, count)}
    group_index = {}

    with open(sparse_path, "rb") as f:
        # Read header information
        (size,) = struct.unpack("@i", f.read(4))
        groups = struct.unpack(f"@{size}i", f.read(4 * size))
        dims = struct.unpack(f"@{size}i", f.read(4 * size))
        counts = struct.unpack(f"@{size}q", f.read(8 * size))

        # Initialize data structures
        for group, dim, count in zip(groups, dims, counts):
            group_info[group] = (dim, count)
            group_data[group] = np.zeros((count + 1, dim), dtype=np.float32)
            group_ids[group] = {}
            group_index[group] = 0

        # Process data records
        while True:
            key_bytes = f.read(8)
            if not key_bytes:
                break

            (key,) = struct.unpack("@q", key_bytes)
            (group,) = struct.unpack("@i", f.read(4))
            dim, count = group_info[group]

            weights = np.frombuffer(f.read(4 * dim), dtype=np.float32)
            index = group_index[group] + 1  # Index starts at 1 (0 reserved for padding)
            group_data[group][index] = weights
            group_ids[group][key] = index
            group_index[group] = index

    return group_data, group_ids


def save_model_for_inference(model: torch.nn.Module, output_dir: str, **kwargs) -> None:
    """!
    @brief Serializes optimized model for inference using ONNX format

    @details Performs the following operations:
        1. Cleans and creates output directory
        2. Exports sparse data to a temporary file
        3. Replaces embedding layers in the model
        4. Saves model in ONNX format
        5. Cleans up temporary files

    @param model PyTorch model to serialize
    @param output_dir Output directory path
    @param kwargs Additional arguments including:
        - dummy_input: Dummy input for the model
        - input_names: Names of the model inputs
        - output_names: Names of the model outputs
        - dynamic_axes: Dynamic axes information for the model

    @throws ValueError if input, input_names, or output_names are not provided

    @warning This function modifies the original model structure - always pass a deepcopy

    @note Output directory structure:
        - inference/
            |- model.onnx       # ONNX model
            |- mapper.json      # Index mapping table
    """
    if (
        "dummy_input" not in kwargs
        or "input_names" not in kwargs
        or "output_names" not in kwargs
    ):
        raise ValueError(
            "Missing required dummy_input, input_names, or output_names in kwargs."
        )

    output_path = Path(output_dir) / "inference"

    # Clean and create output directory
    if output_path.exists():
        shutil.rmtree(output_path)
    output_path.mkdir(parents=True)

    # Generate and process sparse data
    sparse_path = output_path / "sparse.dat"
    dump(str(sparse_path))

    group_data, group_ids = sparse_to_numpy(str(sparse_path))
    model_copy = copy.deepcopy(model)

    def replace_embeddings(module: torch.nn.Module) -> None:
        """!
        @brief Recursively replaces embedding layers

        @param module Current module being processed
        """
        for name, child in module.named_children():
            if isinstance(child, Embedding):
                data = group_data[child.group]
                new_embedding = torch.nn.Embedding(
                    num_embeddings=data.shape[0],
                    embedding_dim=data.shape[1],
                    padding_idx=0,
                    _weight=torch.from_numpy(data).clone().detach(),
                )
                new_embedding.requires_grad_(False)
                setattr(module, name, new_embedding)
            else:
                replace_embeddings(child)

    replace_embeddings(model_copy)

    # Save processed model in ONNX format
    script_model = torch.jit.script(model_copy)
    dummy_input = kwargs.get("dummy_input")
    input_names = kwargs.get("input_names")
    output_names = kwargs.get("output_names")
    dynamic_axes = kwargs.get("dynamic_axes")

    torch.onnx.export(
        script_model,
        dummy_input,
        output_path / "model.onnx",
        export_params=True,
        opset_version=10,
        do_constant_folding=True,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
    )

    # Save metadata
    with (output_path / "mapper.json").open("w", encoding="utf-8") as f:
        json.dump(group_ids, f)

    sparse_path.unlink()  # Cleanup temporary file
